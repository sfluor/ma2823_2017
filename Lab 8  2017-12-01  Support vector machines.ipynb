{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#  2017-12-01  SVMs\n",
    "\n",
    "In this lab, we will apply support vector classification methods to the Endometrium vs. Uterus cancer data. For documentation see: http://scikit-learn.org/0.17/modules/svm.html#svm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load data\n",
    "\n",
    "We will start as usual with library imports and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import data from file\n",
    "df_data = pd.read_csv('./data/small_Endometrium_Uterus.csv')\n",
    "\n",
    "# create matrix of numeric values\n",
    "X = df_data.drop(['ID_REF', 'Tissue'], axis=1).as_matrix()\n",
    "\n",
    "# extract class column\n",
    "y = (df_data['Tissue'] == 'Uterus').as_matrix().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print data dimensions\n",
    "print('No. samples: %d\\nNo. features: %d' % X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualise data in two dimensions\n",
    "\n",
    "We will start by visualising the data projected onto the first two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# scale the data\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "NUM_COMPONENTS = 2\n",
    "\n",
    "pca_model = PCA(n_components=NUM_COMPONENTS)\n",
    "Z = pca_model.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "plt.scatter(Z[y==0, 0], Z[y==0, 1], label='Endometrium', color='blue')\n",
    "plt.scatter(Z[y==1, 0], Z[y==1, 1], label='Uterus', color='orange')\n",
    "plt.xlabel('PC1') ; plt.ylabel('PC2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1. Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 SVM decision function\n",
    "\n",
    "SVMs do not naturally compute probabilities. It is possible to convert the output of the decision function into a probability, but that is a computationally intensive procedure, called Platt's scaling. You can read about it in the corresponding paper: https://www.microsoft.com/en-us/research/publication/probabilities-for-sv-machines/.\n",
    "\n",
    "The natural way for SVMs to return scores (and not predicted classes) is to use the output of their decision function directly.\n",
    "\n",
    "**Question:** Modify the `cross_validate` function to return as predictions the values of the `svm.decision_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_validate(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns, for each data point x, \n",
    "    the value of the decision function f computed when x was part of the test set. \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - decision_function(X) to apply the trained classifier to the data X \n",
    "        and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "\n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape) # Hold all predictions, in correct order.\n",
    "    for tr, te in cv_folds:\n",
    "        # Restrict data to train/test folds\n",
    "        Xtr = design_matrix[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = design_matrix[te, :]\n",
    "\n",
    "        # Fit classifier\n",
    "        classifier.fit(Xtr, ytr)\n",
    "\n",
    "        # Compute decision function on test data\n",
    "        # TODO\n",
    "\n",
    "        # Update pred \n",
    "        pred[te] = yte\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Linear SVM with default C value\n",
    "\n",
    "Let us cross-validate an SVM with linear kernel (linear soft-margin SVM) with default C parameter.\n",
    "\n",
    "**N.B.** the k-fold `split()` function creates a generator that will have to be initialised each time. We therefore store the folds in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set up a stratified 10-fold cross-validation\n",
    "from sklearn import model_selection\n",
    "skf = model_selection.StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X, y)\n",
    "folds = [(tr,te) for (tr,te) in skf.split(X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross-validate a linear SVM with default parameter C\n",
    "clf = svm.SVC(kernel='linear')\n",
    "ypred_linear = cross_validate(X, y, clf, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** Plot the corresponding ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, ypred_linear)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(fpr, tpr,\n",
    "         color='orange',\n",
    "         label='Linear SVM (AUC=%.2f)' % auc)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=10)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=10)\n",
    "ax.set_title('ROC curve', fontsize=10)\n",
    "plt.legend(loc='lower right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** Why is the result so poor? *(Hint: look at the PCA plot from earlier)*\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3 Number of support vectors\n",
    "\n",
    "The `n_support_` atritbute of an svm classifier gives us the number of support vectors for each class.\n",
    "\n",
    "**Question:** How many support vectors does our classifier have? How many is this compared to the number of training samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.4 Kernel matrix\n",
    "\n",
    "The kernel matrix is the matrix of size $n \\times n$ that has for entry $K_{ij}$ the value $k(x^i, x^j)$, where $k$ is the kernel function used.\n",
    "\n",
    "In the case of the linear kernel, the kernel function is simply the inner product.\n",
    "\n",
    "**Question**: Plot the matrix $\\mathbf{K}$ for the linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kmatrix = # TODO\n",
    "\n",
    "# heatmap + color map\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot = ax.imshow(kmatrix, cmap=plt.cm.PuRd) \n",
    "\n",
    "# set axes boundaries\n",
    "ax.set_xlim([0, X.shape[0]]) ; ax.set_ylim([0, X.shape[0]])\n",
    "\n",
    "# flip the y-axis\n",
    "ax.invert_yaxis() ; ax.xaxis.tick_top()\n",
    "\n",
    "# plot colorbar to the right\n",
    "plt.colorbar(plot, pad=0.1, fraction=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** What do you observe about the values taken by the kernel?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's scale the data before computing the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_scaled = # TODO\n",
    "kmatrix = # TODO\n",
    "\n",
    "# heatmap + color map\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot = ax.imshow(kmatrix, cmap=plt.cm.PuRd) \n",
    "\n",
    "# set axes boundaries\n",
    "ax.set_xlim([0, X.shape[0]]) ; ax.set_ylim([0, X.shape[0]])\n",
    "\n",
    "# flip the y-axis\n",
    "ax.invert_yaxis() ; ax.xaxis.tick_top()\n",
    "\n",
    "# plot colorbar to the right\n",
    "plt.colorbar(plot,  pad=0.1, fraction=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** What is the effect of scaling?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's see how scaling affects the performance of the linear SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_with_scaling(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns, for each data point x, \n",
    "    the value of the decision function f computed when x was part of the test set. \n",
    "    \n",
    "    Scale the training data, and apply same scaling to the test data.\n",
    "   \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - decision_function(X) to apply the trained classifier to the data X \n",
    "        and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        # Restrict data to train/test folds\n",
    "        Xtr = design_matrix[tr, :]\n",
    "        ytr = labels[tr]\n",
    "        Xte = design_matrix[te, :]\n",
    "        \n",
    "        # Create scaler object\n",
    "        # TODO\n",
    "        \n",
    "        # Fit the scaler and transform training data\n",
    "        # TODO\n",
    "        \n",
    "        # Transform test data\n",
    "        # TODO\n",
    "        \n",
    "        # Fit classifier\n",
    "        # TODO\n",
    "\n",
    "        # Compute decision function on test data (as before)\n",
    "        # TODO\n",
    "        \n",
    "        # Update pred\n",
    "        # TODO\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear') \n",
    "ypred_linear_scaled = cross_validate_with_scaling(X, y, clf, folds)\n",
    "\n",
    "fpr2, tpr2, thresholds2 = metrics.roc_curve(y, ypred_linear_scaled)\n",
    "auc2 = metrics.auc(fpr2, tpr2)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(fpr, tpr,\n",
    "         color='orange',\n",
    "         label='Linear SVM (AUC=%.2f)' % auc)\n",
    "\n",
    "plt.plot(fpr2, tpr2,\n",
    "         label='Linear SVM + scaling (AUC=%.2f)' % auc2)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=10)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=10)\n",
    "ax.set_title('ROC curve', fontsize=10)\n",
    "plt.legend(loc='lower right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** Now optimise for the C-parameter within each loop of the cross-validation. Plot the new ROC curve (you should set the GridSearchCV scoring parameter so that it will create the roc curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_dict = {'C': np.logspace(-4, 2, 7)}\n",
    "\n",
    "# Initialize a linear SVM object that will optimize C by grid search\n",
    "clf_C = # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cross-validate the linear SVM with optimal C\n",
    "ypred_linear_scaled = cross_validate_with_scaling(X, y, clf_C, folds)\n",
    "\n",
    "fpr2s, tpr2s, thresholds2s = metrics.roc_curve(y, ypred_linear_scaled)\n",
    "auc2s = metrics.auc(fpr2s, tpr2s)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(fpr, tpr, #TODO\n",
    "         label='Linear SVM (AUC=%.2f)' % auc)\n",
    "\n",
    "plt.plot(fpr2, tpr2, #TODO\n",
    "         label='Linear SVM + scaling (AUC=%.2f)' % auc2)\n",
    "\n",
    "plt.plot(fpr2, tpr2,\n",
    "         label='Linear SVM + scaling (opt. C) (AUC=%.2f)' % auc2)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=10)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=10)\n",
    "ax.set_title('ROC curve', fontsize=10)\n",
    "plt.legend(loc='lower right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2. Polynomial kernels\n",
    "\n",
    "We will use SVMs with kernels of the form $k(x, x') = (\\langle x, x' \\rangle + r)^d$.\n",
    "\n",
    "**Question**: Plot kernel matrices for $r=0, d=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# normalise data\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# Construct kernel matrix with r = 0 and d = 2\n",
    "r = 0\n",
    "d = 2\n",
    "\n",
    "# Form the kernel matrix\n",
    "kmatrix = # TODO\n",
    "\n",
    "# heatmap + color map\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot = ax.imshow(kmatrix, cmap=plt.cm.PuRd)\n",
    "\n",
    "# set axes boundaries\n",
    "ax.set_xlim([0, X.shape[0]]) ; ax.set_ylim([0, X.shape[0]])\n",
    "\n",
    "# flip the y-axis\n",
    "ax.invert_yaxis() ; ax.xaxis.tick_top()\n",
    "\n",
    "# plot colorbar to the right\n",
    "plt.colorbar(plot, pad=0.1, fraction=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** What do you observe? What is going to happen if you increase $d$? How do you think this will affect the SVM? Cross-validate the SVM and plot the ROC curve.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=d, coef0=r)\n",
    "ypred_quadratic = cross_validate_with_scaling(X, y, clf, folds)\n",
    "fpr3, tpr3, thresholds3 = metrics.roc_curve(y, ypred_quadratic)\n",
    "auc3 = metrics.auc(fpr3, tpr3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.plot(fpr2s, tpr2s, #TODO\n",
    "         label='Linear SVM + scaling (opt. C) (AUC=%.2f)' % auc2s)\n",
    "\n",
    "plt.plot(fpr3, tpr3,\n",
    "         label='Quadratic SVM + scaling (AUC=%.2f)' % auc3)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=10)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=10)\n",
    "ax.set_title('ROC curve', fontsize=10)\n",
    "plt.legend(loc='lower right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question**: What value for $r$ can change this behavior? Plot the corresponding kernel matrix.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "# Set the r parameter to something suitable\n",
    "r =  # TODO\n",
    "d = 2\n",
    "\n",
    "# Form kernel matrix\n",
    "kmatrix = # TODO\n",
    "\n",
    "# heatmap + color map\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot = ax.imshow(kmatrix, cmap=plt.cm.PuRd) \n",
    "\n",
    "# set axes boundaries\n",
    "ax.set_xlim([0, X.shape[0]]) ; ax.set_ylim([0, X.shape[0]])\n",
    "\n",
    "# flip the y-axis\n",
    "ax.invert_yaxis() ; ax.xaxis.tick_top()\n",
    "\n",
    "# plot colorbar to the right\n",
    "plt.colorbar(plot, pad=0.1, fraction=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question**: Now evaluate an SVM with polynomial kernel of degree d=2 and value for r as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='poly', degree=d, coef0=r)\n",
    "ypred_quadratic = cross_validate_with_scaling(X, y, clf, folds)\n",
    "fpr4, tpr4, thresholds4 = metrics.roc_curve(y, ypred_quadratic)\n",
    "auc4 = metrics.auc(fpr4, tpr4)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(fpr2, tpr2,\n",
    "         color='orange',\n",
    "         label='Linear SVM + scaling (AUC=%.2f)' % auc2)\n",
    "\n",
    "plt.plot(fpr4, tpr4,\n",
    "         label='Quadratic SVM (AUC=%.2f)' % auc4)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=10)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=10)\n",
    "ax.set_title('ROC curve', fontsize=10)\n",
    "plt.legend(loc='lower right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3. Gaussian RBF kernels\n",
    "\n",
    "We will use SVMs with kernels of the form $k(x, x') = \\exp \\left(-\\gamma ||x - x'||^2 \\right)$.\n",
    "\n",
    "The following code efficiently computes the pairwise squared distances between all items in X, that is to say the matrix $P$ such that $P_{ij} = ||x^i - x^j||^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "pairwise_sq_dists = squareform(pdist(X, 'sqeuclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question**: Plot kernel matrices for varying values of $\\gamma$. What do you observe? What is going to be the impact on the SVM? What happens with very small values of $\\gamma$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "X_scaled = preprocessing.scale(X)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create list of gamma values to try\n",
    "gamma_values = [] #TODO\n",
    "\n",
    "for i, gamma in enumerate(gamma_values):\n",
    "    ax = fig.add_subplot(2, 2, i + 1)\n",
    "    pairwise_sq_dists = rbf_kernel(X_scaled, gamma=gamma)\n",
    "\n",
    "    # heatmap + color map\n",
    "    plot = ax.imshow(pairwise_sq_dists, cmap=plt.cm.PuRd) \n",
    "\n",
    "    # set axes boundaries\n",
    "    ax.set_xlim([0, X.shape[0]]) ; ax.set_ylim([0, X.shape[0]])\n",
    "\n",
    "    # flip the y-axis\n",
    "    ax.invert_yaxis() ; ax.xaxis.tick_top()\n",
    "\n",
    "    # plot colorbar to the right\n",
    "    plt.colorbar(plot)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question:** Compare the performance of an rbf SVM (for a given choice of gamma) with the linear SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create rbf kernel\n",
    "clf = #TODO\n",
    "ypred_rbf = cross_validate_with_scaling(X, y, clf, folds)\n",
    "\n",
    "fpr5, tpr5, thresholds5 = metrics.roc_curve(y, ypred_rbf)\n",
    "auc5 = metrics.auc(fpr5, tpr5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(fpr2, tpr2,\n",
    "         color='orange',\n",
    "         label='Linear SVM + scaling (AUC=%.2f)' % auc2)\n",
    "\n",
    "plt.plot(fpr5, tpr5,\n",
    "         label='RBF SVM (AUC=%.2f)' % auc5)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=10)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=10)\n",
    "ax.set_title('ROC curve', fontsize=10)\n",
    "plt.legend(loc='lower right', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4. Bonus: Coding an SVM\n",
    "\n",
    "We have coded a working SVM with a simplified version of the specialised SMO algorithm (see below). It is based on a tutorial from Stanford course CS229 (http://cs229.stanford.edu/materials/smo.pdf). Since it is a messy algorithm, we will walk through it and ask you to code small parts only. For future reference, recall the form of an SVM in the dual form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{rl}\n",
    "\\max_{\\boldsymbol\\alpha} & W(\\boldsymbol\\alpha) = \\sum_{i = 1}^{N}\\alpha_i + \\frac{1}{2}\\sum_{i}^N\\sum_{j}^N \\alpha_i\\alpha_jy_iy_j\\mathbf{x}_i^T\\mathbf{x}_j \\\\\n",
    "\\text{subject to} & \\sum_{i=1}^N \\alpha_iy_i = 0 \\\\\n",
    "& 0 \\leq \\alpha_i \\leq C, \\forall i\n",
    "\\end{array}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1 SVM Decision function\n",
    "\n",
    "In the dual form, the decision function of an SVM is given as,\n",
    "\n",
    "$$\n",
    "f(\\mathbf{\\mathbf{x}}) = \\sum_{i=1}^N \\alpha_iy_i\\kappa(\\mathbf{x}_i, \\mathbf{x}) + b,\n",
    "$$\n",
    "\n",
    "and a prediction is then made as $\\text{sgn}\\big(f(\\mathbf{x})\\big)$ where $\\text{sgn} : x \\to \\{1, -1\\}$ is the sign function. One can see how an observation $\\mathbf{x}$ is classified according to its similarity with the training vectors. Each training vector, $\\mathbf{x}_i$, casts its vote, $y_i$, be it $-1$ or $1$, with the influence regulated by a similarity measure provided by the kernel function, and the weight provided by the dual variable. Only the support vectors have a (non-zero) weighted vote. It is easy to see why this is such a powerful framework--we can choose between many similarity measures (kernels), though the mathematics restricts us to choosing Mercer kernels.\n",
    "\n",
    "**Question:** Write the `decision_function()` (not the prediction) for an SVM. Assume `Ki` is a precomputed vector giving all kernel values $\\kappa(\\mathbf{x}_i, \\mathbf{x})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def decision_function(Ki, y, a, b):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.2 Sequential Minimal Optimisation\n",
    "\n",
    "Our model may be solved with any QP solver, formerly the standard approach. However, in 1998, American computer scientist, John Platt (1963-), invented sequential minimal optimisation (SMO), an algorithm for analytically optimising the dual variables two at a time. The superiority and simplicity of this algorithm helped bring support vector machines into the forefront of machine learning research. The algorithm works by iteratively optimising pairs of variables that violate the optimality conditions. These are,\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{array}{rcl}\n",
    "\\alpha_i = 0 & \\implies & y_if(\\mathbf{x}_i) \\geq 1 \\notag\\\\\n",
    "0 < \\alpha_i < C  & \\implies & y_if(\\mathbf{x}_i) = 1 \\notag \\\\\n",
    "\\alpha_i = C & \\implies & y_if(\\mathbf{x}_i) \\leq 1 \\notag\n",
    "\\end{array}\n",
    "\\end{align}\n",
    "\n",
    "#### Updating the weights\n",
    "\n",
    "Optimisation is done *in pairs* (this is what *minimal* refers to in SMO) in order to maintain the equality constraint of the problem, $\\sum_{i=1}^N \\alpha_iy_i = 0$ (see above). Thus, for pair $\\alpha_i$ and $\\alpha_j$, we make updates such that $y_i\\Delta\\alpha_i + y_j\\Delta\\alpha_j = 0$. Rearranging gives $\\Delta\\alpha_j = -\\Delta\\alpha_iy^{(i)}/y^{(j)} = -\\Delta\\alpha_iy^{(i)}y^{(j)}$, as since $y_i \\in \\{-1, 1\\}$, division is the same as multiplication, and the cost function becomes,\n",
    "\n",
    "\\begin{align}\n",
    "W(\\boldsymbol\\alpha) &= \\sum_{k=1}^N\\alpha_k + \\Delta\\alpha_i -\\Delta\\alpha_iy_iy_j - \\frac{1}{2}\\sum_{k=1}^Ny_k\\alpha_k\\big(f(\\mathbf{x}_k) - b\\big) \\notag \\\\ \n",
    "&- \\Delta\\alpha_i\\big(f(\\mathbf{x}_i) - f(\\mathbf{x}_j)\\big) - \\Delta\\alpha_iy_i\\big(f(\\mathbf{x}_i) - f(\\mathbf{x}_j)\\big) - \\frac{1}{2}\\Delta\\alpha_i^2\\big(\\kappa(\\mathbf{x}_i, \\mathbf{x}_i) - 2\\kappa(\\mathbf{x}_i, \\mathbf{x}_j) + \\kappa(\\mathbf{x}_j, \\mathbf{x}_j)\\big). \\notag \n",
    "\\end{align}\n",
    "\n",
    "Differentiating and solving gives,\n",
    "\n",
    "$$\\Delta\\alpha_j^* = \\frac{y_j(E_i - E_j)}{2\\kappa(\\mathbf{x}_i, \\mathbf{x}_j) - \\kappa(\\mathbf{x}_i, \\mathbf{x}_i) - \\kappa(\\mathbf{x}_j, \\mathbf{x}_j)},$$\n",
    "\n",
    "where $E_i = f(\\mathbf{x}_i) - y_i$ and $E_j = f(\\mathbf{x}_j) - y_j$ are the errors.\n",
    "\n",
    "**Question:** Write the function to calculate `delta_a()`. Use `i` and `j` to index the kernel matrix. Note that for simplicity, we assume the errors have been calculated already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def delta_a(K, y, i, j, Ei, Ej):\n",
    "    delta_a = # TODO\n",
    "    return delta_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This gives us our update step. The value of $\\Delta\\alpha_j^*$ can then be determined from the initial equality. Note that both variables remain constrained, hence their values must be clipped if they are less than $0$ or greater than $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Updating the bias\n",
    "\n",
    "The bias term is then updated to account for the changes. For example, to ensure the classifier now emits $y_i$ for $f(\\mathbf{x}_i)$, we rearrange,\n",
    "\n",
    "$$y_i = \\sum_{k=1}^N \\alpha_iy_i\\kappa(\\mathbf{x}_k, \\mathbf{x}_i) + b + \\Delta b + \\Delta\\alpha y_i\\kappa(\\mathbf{x}_i, \\mathbf{x}_i) + \\Delta\\alpha y_j\\kappa(\\mathbf{x}_i, \\mathbf{x}_j),$$\n",
    "\n",
    "for $\\Delta b^*$. The same is done for $f(\\mathbf{x}_j)$, and the expressions are combined according to the convergence conditions. Thus, the optimisation steps are done analytically, rather than with linear algebra. The rest of the algorithm is mostly concerned with heuristics for choosing the best $\\alpha_i$, $\\alpha_j$ pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.3 Putting it all together\n",
    "\n",
    "Your functions will be called by the class `SupportVectorMachine` defined below. *(you do not need to change anything, just compile it and move on...)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "class SupportVectorMachine:\n",
    "\n",
    "    def __init__(self, kernel='linear', gamma=0.01, d=2, r=0):\n",
    "\n",
    "        \"\"\" Initialisation function for support vector machine\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        kernel: {linear, rbf, polynomial} string\n",
    "                Indicates the kernel function to use\n",
    "        gamma:  float\n",
    "                Variance of rbf kernel\n",
    "        d:      float\n",
    "                Degree for polynomial kernel\n",
    "        r:      float\n",
    "                Consant for polynomial kernel\n",
    "        \"\"\"\n",
    "        if kernel == 'linear':\n",
    "            self.kernel = lambda x, y : x.dot(y.T)\n",
    "        elif kernel == 'rbf':\n",
    "            self.kernel = lambda x, y : rbf_kernel(x, y, gamma=0.01)\n",
    "        elif kernel == 'polynomial':\n",
    "            self.kernel = lambda x, y : (x.dot(y.T) + r) ** d\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        \"\"\" Prediction function for support vector machine\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x:      (1, n_features) np.array\n",
    "                Test sample\n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        prediction: {-1, 1}\n",
    "            Sign of output of linear score function\n",
    "        \"\"\"\n",
    "        # calculate kernel values for input x\n",
    "        kx = self.kernel(self.Xtr, x.reshape(1, -1))\n",
    "        return np.sign(self.a.dot(np.diag(ytr)).dot(kx) + self.b)\n",
    "\n",
    "    def decision_function(self, Ki):\n",
    "\n",
    "        \"\"\" Decision function for support vector machine\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x:      (1, n_features) np.array\n",
    "                Test sample\n",
    "\n",
    "        Return:\n",
    "        -------\n",
    "        prediction: float\n",
    "            Output of linear score function\n",
    "        \"\"\"\n",
    "        return decision_function(Ki, self.ytr, self.a, self.b)\n",
    "\n",
    "    def fit(self, Xtr, ytr, C=0.01, tol=1e-5, max_iters=100):\n",
    "\n",
    "        \"\"\" Training algorithm for support vector machine.\n",
    "        \n",
    "        SMO sequential minimal optimisation algorithm for training. \n",
    "        This is the simplified version (without heuristics) detailed in\n",
    "        cs229.stanford.edu/materials/smo.pdf\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Xtr:       (n_samples, n_features) np.array\n",
    "                   First data matrix.\n",
    "        ytr:       (n_samples, 1) np.array\n",
    "                   Second data matrix.\n",
    "        C:         float\n",
    "                   margin size\n",
    "        tol:       float\n",
    "                   precision on optimality (stopping condition)\n",
    "        max_iters: int\n",
    "                   maximum number of optimisation passes\"\"\"\n",
    "\n",
    "        self.Xtr = Xtr\n",
    "        self.ytr = ytr\n",
    "\n",
    "        N = self.Xtr.shape[0]\n",
    "        K = self.kernel(self.Xtr, self.Xtr)  # kernel matrix\n",
    "\n",
    "        self.a = np.zeros(N)  # support vector weights\n",
    "        self.b = 0  # bias term\n",
    "\n",
    "        iters = 0\n",
    "\n",
    "        while iters < max_iters:\n",
    "            num_changed = 0\n",
    "            # iterate over samples\n",
    "            for i in range(N):\n",
    "                # Calculate error\n",
    "                Ei = self.decision_function(K[i]) - self.ytr[i]\n",
    "                # Check optimality constraints\n",
    "                \n",
    "                if (self.ytr[i] * Ei < -tol and self.a[i] < C) or \\\n",
    "                   (self.ytr[i] * Ei > +tol and self.a[i] > 0):\n",
    "\n",
    "                    # Pick random aj\n",
    "                    j = np.random.choice(filter(lambda x : x != i, range(N)))\n",
    "                    Ej = self.decision_function(K[j]) - self.ytr[j]\n",
    "\n",
    "                    # Record ai, aj\n",
    "                    ai_old = self.a[i]\n",
    "                    aj_old = self.a[j]\n",
    "\n",
    "                    # Set bounds\n",
    "                    L = 0 ; H = 0\n",
    "                    if self.ytr[i] != self.ytr[j]:\n",
    "                        L = max(0, self.a[j] - self.a[i])\n",
    "                        H = min(C, C + self.a[j] - self.a[i])\n",
    "                    else:\n",
    "                        L = max(0, self.a[i] + self.a[j] - C)\n",
    "                        H = min(C, self.a[i] + self.a[j])\n",
    "\n",
    "                    if L == H:\n",
    "                        continue\n",
    "    \n",
    "                    # Update aj\n",
    "                    self.a[j] -= delta_a(K, self.ytr, i, j, Ei, Ej)\n",
    "\n",
    "                    # Clip value\n",
    "                    self.a[j] = max(min(self.a[j], H), L)\n",
    "\n",
    "                    # Check for change\n",
    "                    if abs(self.a[j] - aj_old) < tol:\n",
    "                        continue\n",
    "\n",
    "                    # Update ai\n",
    "                    self.a[i] += self.ytr[i] * self.ytr[j] * (aj_old - self.a[j])\n",
    "\n",
    "                    # Set bias term\n",
    "                    b1 = self.b - Ei - \\\n",
    "                         self.ytr[i] * (self.a[i] - ai_old) * K[i, i] - \\\n",
    "                         self.ytr[j] * (self.a[j] - aj_old) * K[i, j]\n",
    "\n",
    "                    b2 = self.b - Ej - \\\n",
    "                         self.ytr[i] * (self.a[i] - ai_old) * K[i, j] - \\\n",
    "                         self.ytr[j] * (self.a[j] - aj_old) * K[j, j]\n",
    "\n",
    "                    if 0 < self.a[i] and self.a[i] < C:\n",
    "                        self.b = b1\n",
    "                    elif 0 < self.a[j] and self.a[j] < C:\n",
    "                        self.b = b2\n",
    "                    else:\n",
    "                        self.b = float(b1 + b2) / 2\n",
    "\n",
    "                    num_changed += 1\n",
    "\n",
    "            iters = iters + 1 if num_changed == 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.4 Linearly separable data\n",
    "\n",
    "We will now test our implementation on some toy data. We being by creating a random dataset sampled from two distinct Gaussians:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "X = np.concatenate((multivariate_normal(mean=np.array([1, 1]),\n",
    "                                        cov=np.array([[1, 0], [0, 1]]),\n",
    "                                        size=num_samples),\n",
    "                    multivariate_normal(mean=np.array([-1, -1]),\n",
    "                                        cov=np.array([[1, 0], [0, 1]]),\n",
    "                                        size=num_samples)))\n",
    "\n",
    "y = np.array(num_samples * [1] + num_samples * [-1])\n",
    "\n",
    "# visualise mesh as contour plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_xlabel('x1') ; ax.set_ylabel('x2')\n",
    "\n",
    "# plot training data\n",
    "plt.scatter(X[:num_samples, 0], X[:num_samples, 1], color='orange')\n",
    "plt.scatter(X[num_samples:, 0], X[num_samples:, 1], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's train our custom SVM with a linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y)\n",
    "\n",
    "# train linear SVM\n",
    "clf = SupportVectorMachine(kernel='linear')\n",
    "clf.fit(Xtr, ytr, C=0.5, max_iters=100)\n",
    "\n",
    "# calculate accuracy on test set\n",
    "linear_pred = np.array([clf.predict(xte) for xte in Xte]).reshape(yte.shape[0])\n",
    "acc = float(sum(linear_pred==yte)) / yte.shape[0]\n",
    "print('Accuracy (linear): %.00f%%' % (100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualise decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create mesh coordinates\n",
    "lim = 5\n",
    "xx, yy = np.meshgrid(np.arange(-lim, +lim, 0.1),\n",
    "                     np.arange(-lim, +lim, 0.1))\n",
    "zz = np.zeros(xx.shape)\n",
    "\n",
    "# create mesh of predictions\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        zz[i, j] = clf.predict(np.array([xx[i, j], yy[i, j]]))\n",
    "\n",
    "# visualise mesh as contour plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_xlabel('x1') ; ax.set_ylabel('x2')\n",
    "ax.set_xlim([-lim, +lim-0.1]) ; ax.set_ylim([-lim, +lim-0.1])\n",
    "ax.contourf(xx, yy, zz, alpha=0.5, colors=('blue', 'orange'))\n",
    "\n",
    "# plot training data\n",
    "plt.scatter(X[:num_samples, 0], X[:num_samples, 1], color='orange')\n",
    "plt.scatter(X[num_samples:, 0], X[num_samples:, 1], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "No problem! Now for something a little more challenging..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.5 Non-linear decision boundaries\n",
    "\n",
    "Now we will try data that is not linearly separable: a circle inside a square! We will see that our linear kernel is insufficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "\n",
    "num_samples = 200\n",
    "\n",
    "X = np.random.uniform(low=-1, high=1, size=(num_samples, 2))\n",
    "y = np.array([1 if np.sqrt(np.dot(x, x.T)) < 1 / np.sqrt(2) else -1 for x in X])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_xlabel('x1') ; ax.set_ylabel('x2')\n",
    "\n",
    "class_pos = filter(lambda i : y[i] == +1,  np.arange(X.shape[0]))\n",
    "class_neg = filter(lambda i : y[i] == -1,  np.arange(X.shape[0]))\n",
    "\n",
    "# plot training data\n",
    "plt.scatter(X[class_pos, 0], X[class_pos, 1], color='orange')\n",
    "plt.scatter(X[class_neg, 0], X[class_neg, 1], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Uh oh! Polynomial kernels to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create train-test split\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y)\n",
    "\n",
    "# train linear SVM\n",
    "clf_linear = SupportVectorMachine(kernel='linear')\n",
    "clf_linear.fit(Xtr, ytr, C=0.1, max_iters=100)\n",
    "\n",
    "# calculate accuracy on test set\n",
    "linear_pred = np.array([clf_linear.predict(xte) for xte in Xte]).reshape(yte.shape[0])\n",
    "acc = float(sum(linear_pred==yte)) / yte.shape[0]\n",
    "print('Accuracy (linear): %.00f%%' % (100 * acc))\n",
    "\n",
    "# train quadratic kernel SVM\n",
    "clf_poly = #TODO\n",
    "\n",
    "# calculate accuracy on test set\n",
    "quad_pred = np.array([clf_poly.predict(xte) for xte in Xte]).reshape(yte.shape[0])\n",
    "acc = float(sum(quad_pred==yte)) / yte.shape[0]\n",
    "print('Accuracy (quadratic): %.00f%%' % (100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We plot the decision boundaries for the quadratic kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create mesh coordinates\n",
    "lim = 1\n",
    "xx, yy = np.meshgrid(np.arange(-lim, +lim, 0.1), np.arange(-lim, +lim, 0.1))\n",
    "zz_poly = np.zeros(xx.shape)\n",
    "\n",
    "# create mesh of predictions\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        zz_poly[i, j] = clf_poly.predict(np.array([xx[i, j], yy[i, j]]))\n",
    "\n",
    "# visualise mesh as contour plot\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_xlabel('x1') ; ax.set_ylabel('x2')\n",
    "ax.set_xlim([-lim, +lim-0.1]) ; ax.set_ylim([-lim, +lim-0.1])\n",
    "ax.contourf(xx, yy, zz_poly, alpha=0.5, colors=('blue', 'orange'))\n",
    "\n",
    "# plot training data\n",
    "ax.scatter(X[class_pos, 0], X[class_pos, 1], color='orange')\n",
    "ax.scatter(X[class_neg, 0], X[class_neg, 1], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The data is linearly separable in quadratic space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.6 Non-linear decision boundaries: learning XOR\n",
    "\n",
    "A classic challenge to the power of a classifier is data arranged in an XOR pattern. Let us begin by defining data approximating and XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "\n",
    "num_samples = 50\n",
    "\n",
    "X = np.concatenate((multivariate_normal(mean=np.array([2, 2]),\n",
    "                                        cov=np.array([[1, 0], [0, 1]]),\n",
    "                                        size=num_samples),\n",
    "                    multivariate_normal(mean=np.array([-2, -2]),\n",
    "                                        cov=np.array([[1, 0], [0, 1]]),\n",
    "                                        size=num_samples),\n",
    "                    multivariate_normal(mean=np.array([2, -2]),\n",
    "                                        cov=np.array([[1, 0], [0, 1]]),\n",
    "                                        size=num_samples),\n",
    "                    multivariate_normal(mean=np.array([-2, 2]),\n",
    "                                        cov=np.array([[1, 0], [0, 1]]),\n",
    "                                        size=num_samples)))\n",
    "\n",
    "y = np.array(2 * num_samples * [1] + 2 * num_samples * [-1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_xlabel('x1') ; ax.set_ylabel('x2')\n",
    "\n",
    "class_pos = filter(lambda i : y[i] == +1,  np.arange(X.shape[0]))\n",
    "class_neg = filter(lambda i : y[i] == -1,  np.arange(X.shape[0]))\n",
    "\n",
    "# plot training data\n",
    "plt.scatter(X[class_pos, 0], X[class_pos, 1], color='orange')\n",
    "plt.scatter(X[class_neg, 0], X[class_neg, 1], color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's see how a linear kernel performs against an rbf kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create train-test split\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y)\n",
    "\n",
    "# train linear SVM\n",
    "clf_linear = SupportVectorMachine(kernel='linear')\n",
    "clf_linear.fit(Xtr, ytr, C=0.1, max_iters=100)\n",
    "\n",
    "# calculate accuracy on test set\n",
    "linear_pred = np.array([clf_linear.predict(xte) for xte in Xte]).reshape(yte.shape[0])\n",
    "acc = float(sum(linear_pred==yte)) / yte.shape[0]\n",
    "print('Accuracy (linear): %.00f%%' % (100 * acc))\n",
    "\n",
    "# train rbf svm\n",
    "clf_rbf = # TODO\n",
    "\n",
    "# calculate accuracy on test set\n",
    "quad_pred = np.array([clf_rbf.predict(xte) for xte in Xte]).reshape(yte.shape[0])\n",
    "acc = float(sum(quad_pred==yte)) / yte.shape[0]\n",
    "print('Accuracy (rbf): %.00f%%' % (100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, we visualise the rbf-SVM decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create mesh coordinates\n",
    "lim = 4\n",
    "xx, yy = np.meshgrid(np.arange(-lim, +lim, 0.1), np.arange(-lim, +lim, 0.1))\n",
    "zz_rbf = np.zeros(xx.shape)\n",
    "\n",
    "# create mesh of predictions\n",
    "for i in range(xx.shape[0]):\n",
    "    for j in range(xx.shape[1]):\n",
    "        zz_rbf[i, j] = clf_rbf.predict(np.array([xx[i, j], yy[i, j]]))\n",
    "\n",
    "# visualise mesh as contour plot\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_xlabel('x1') ; ax.set_ylabel('x2')\n",
    "ax.set_xlim([-lim, +lim-0.1]) ; ax.set_ylim([-lim, +lim-0.1])\n",
    "ax.contourf(xx, yy, zz_rbf, alpha=0.5, colors=('blue', 'orange'))\n",
    "\n",
    "# plot training data\n",
    "ax.scatter(X[class_pos, 0], X[class_pos, 1], color='orange')\n",
    "ax.scatter(X[class_neg, 0], X[class_neg, 1], color='blue')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
