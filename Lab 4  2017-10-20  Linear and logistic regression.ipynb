{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2017-10-20  Linear and logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to explore linear and logistic regression, implement them yourself and learn to use their respective scikit-learn implementation.\n",
    "\n",
    "Let us start by loading some of the usual librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, metrics\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. (Vanilla) Gradient descent\n",
    "To optimise our error while doing linear regression, we will use gradient descent. \n",
    "\n",
    "__Reminder about gradient descent:__ This algorithm is commonly used to optimise convex functions. The idea is very simple â€” to reach the global optimum of a convex function from any point, we need to move in the direction opposite to that of greatest increase of the function. As the function is convex, this strategy will always take us to the global optimum. \n",
    "\n",
    "Now, the direction of greatest increase of the function is determined by taking the partial derivatives of the function with respect to every variable. For example, let us say we wish to optimise a convex function $f(x)$, where $x = (x_1, x_2, \\ldots, x_d)^T$. Let us say, we start at a point $s \\in \\mathbb{R}^d$. Then, the direction of greatest increase of $f$ at $s$ is given by \n",
    "\n",
    "$$\n",
    "\\nabla f(s) = \\left(\\frac{\\partial f(s)}{\\partial x_1}, \\frac{\\partial f(s)}{\\partial x_2}, \\ldots ,\\frac{\\partial f(s)}{\\partial x_d} \\right)^T.\n",
    "$$\n",
    "\n",
    "With this derivative, we design an update rule, which asks us to move in the direction opposite to the direction of greatest increase. By repeatedly applying this rule, we hope to reach the global minimum. We hence move to $t$, which is given by\n",
    "\n",
    "$$\n",
    "t = s - \\gamma \\nabla f(s),\n",
    "$$\n",
    "where $\\gamma$ is a parameter called the *learning rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct a class for the gradient descent optimiser. This is a generic class that tries to optimise any function that is given to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall use the power of Python to declare our class. Python supports *first-class functions*, which allows us to pass functions to and return function from other functions. We shall use this to define the gradient descent optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Fill in the `GradientDescentOptimizer` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimizer():\n",
    "    \"\"\" Class for optimization by gradient descent.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    f: function\n",
    "        the function to optimize\n",
    "    fprime: function \n",
    "        the gradient of the function to optimize\n",
    "    beta: np.array\n",
    "        the point where the function is currently being evaluated\n",
    "    lr: float\n",
    "        the learning rate\n",
    "    fx: float\n",
    "        the current function value\n",
    "    fgx: np.array\n",
    "        the current gradient value\n",
    "    beta_history: list of np.array\n",
    "        the list of all points where f has been evaluated\n",
    "    \"\"\"\n",
    "    def __init__(self, f, fprime, start, lr=1e-1):\n",
    "        \"\"\"     \n",
    "        Parameters:\n",
    "        -----------\n",
    "        f: function\n",
    "            the function to optimize \n",
    "            Yes, we can pass functions as parameters! To call f within the code, use f followed \n",
    "            by its arguments enclosed between parentheses, as you would normally do: f(x)\n",
    "        fprime: function\n",
    "            the function's gradient\n",
    "        start: np.array\n",
    "            the starting point, at which we begin our search\n",
    "        lr: float\n",
    "            the learning rate\n",
    "        \"\"\"\n",
    "        # Store the parameters as attributes\n",
    "        self.f      = f\n",
    "        self.fprime = fprime\n",
    "        self.beta   = start\n",
    "        self.lr     = lr\n",
    "        # Save history as attributes\n",
    "        self.beta_history = [start]\n",
    "    \n",
    "    def compute_fprime(self):\n",
    "        \"\"\" Compute the value of the gradient of f for our current point. \n",
    "        Update self.fgx accordingly.\n",
    "        \"\"\"\n",
    "        # Compute the gradient of f for the current parameters. \n",
    "        # Simply call fprime on the current values of the parameters.\n",
    "        # Does not return anything, but simply stores the gradient of f.\n",
    "        #    in self.fgx.\n",
    "        self.fgx = self.fprime(self.beta)\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\" Take a gradient descent step. \n",
    "        Upgrade self.beta accordingly. \n",
    "        \"\"\"\n",
    "        # Take a gradient descent step.\n",
    "        # 1. Compute the gradient at the current parameters. \n",
    "        # 2. Compute the change in beta from this gradient.\n",
    "        # 3. Update self.beta appropriately using the learning rate.\n",
    "        # Does not return anything.\n",
    "        # TODO\n",
    "        self.compute_fprime()\n",
    "        self.beta = self.beta - self.lr * self.fgx\n",
    "        \n",
    "\n",
    "        \n",
    "    def optimize(self, max_iter=100):\n",
    "        \"\"\"Use the gradient descent optimiser to optimise f.\n",
    "        Update self.f_history and self.beta_history accordingly.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_iter: int\n",
    "            Maximum number of iterations.        \n",
    "        \"\"\"\n",
    "        # Use the gradient descent optimiser to optimise f.\n",
    "        # 1. In every iteration, take a gradient descent step.\n",
    "        # 2. Update self.beta_history.\n",
    "        # Does not return anything.\n",
    "        for i in range(max_iter):\n",
    "            self.step()\n",
    "            self.beta_history.append(self.beta)\n",
    "            \n",
    "    def print_result(self):\n",
    "        \"\"\" Print out result once optimization is complete.        \n",
    "        \"\"\"\n",
    "        sys.stdout.write(\" === Result ===\\n\")\n",
    "        sys.stdout.write(\"Best beta found: \" + str(self.beta) +'\\n')\n",
    "        sys.stdout.write(\"f(best beta) = \" + str(self.f(self.beta)) + '\\n')\n",
    "        sys.stdout.write(\"f\\'(best beta) = \" + str(self.fprime(self.beta)) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to optimise a simple function with this optimizer. Define a function $f(x) = \\left(x - \\begin{pmatrix}5 \\\\ 4\\end{pmatrix}\\right)^2$. Note that the input to $f$ is a vector of size 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Fill in the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.linalg.norm(x - np.array([5, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define another function which is the gradient of $f$ at a point $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fprime(x):\n",
    "    return 2*(np.array([x[0]-5, x[1]-4 ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check that f and fprime do what you want them to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "[ 0 -8]\n"
     ]
    }
   ],
   "source": [
    "print(f(np.array([5,0])))\n",
    "print(fprime(np.array([5,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialise a `GradientDescentOptimizer` using this function and its gradient. Run the optimise by calling `.optimize()`. Also, call `gd.print_result()` to see how the optimisation turned out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Result ===\n",
      "Best beta found: [ 5.  4.]\n",
      "f(best beta) = 1.48810340818e-09\n",
      "f'(best beta) = [ -2.20182272e-09  -2.00244443e-09]\n"
     ]
    }
   ],
   "source": [
    "gd = GradientDescentOptimizer(f, fprime, start=np.random.normal(size=(2,), loc=0.0, scale=1.0), lr=1e-1)\n",
    "# TODO\n",
    "gd.optimize()\n",
    "gd.print_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the progression of the value of `f` and see how it goes.\n",
    "Plot also the norm of the gradient at each point (Hint: use the `beta_history` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f976e50abe0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBYSc8YwKKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f979775e438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "f_v, = plt.plot([gd.f(x) for x in gd.beta_history])\n",
    "g_v, = plt.plot([np.linalg.norm(gd.fprime(x)) for x in gd.beta_history])\n",
    "plt.legend([f_v, g_v], ['f(x)', '||f\\'(x)||**2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear regression\n",
    "\n",
    "We will now implement a linear regression, first using the closed form solution, and second with our gradient descent.\n",
    "\n",
    "## 1.1 Linear regression data\n",
    "\n",
    "Our first data set regards the quality ratings of a white _vinho verde_. Each wine is described by a number of physico-chemical descriptors such as acidity, sulfur dioxide content, density or pH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the regression task data\n",
    "wine_data = pd.read_csv('data/winequality-white.csv', sep=\";\")\n",
    "wine_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data into X and y data arrays\n",
    "X_regr = wine_data.drop(['quality'], axis=1).values\n",
    "y_regr = wine_data['quality'].values\n",
    "# Standardize the data\n",
    "sc = preprocessing.StandardScaler()\n",
    "sc.fit(X_regr)\n",
    "X_regr = sc.transform(X_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cross-validation\n",
    "\n",
    "Let us create a cross-validation utility function (similar to what we have done in Lab 3, but for regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# set up folds for cross_validation\n",
    "from sklearn import cross_validation\n",
    "folds_regr = cross_validation.KFold(y_regr.size, n_folds=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_regr(design_matrix, labels, regressor, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    regressor:  Regressor instance; must have the following methods:\n",
    "        - fit(X, y) to train the regressor on the data X, y\n",
    "        - predict(X) to apply the trained regressor to the data X and return estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        regressor.fit(design_matrix[tr,:], labels[tr])\n",
    "        pred[te] = (regressor.predict(design_matrix[te,:]))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Linear regression implementation\n",
    "\n",
    "### Closed-form solution\n",
    "\n",
    "For an input vector $X^T = (X_1 , X_2 , \\dots , X_p )$, and a real-valued output Y, the linear regression model\n",
    "has the form $$f(X) = \\beta_0 + \\sum_{j=1}^pX_j\\beta_j$$\n",
    "We consider a set of training data $(x_1 , y_1 ) \\dots (x_N , y_N )$ from which to estimate the parameters $\\beta$.\n",
    "\n",
    "The most popular estimation method is least squares, in which the coefficients $\\beta = (\\beta_0 , \\beta_1 , \\dots , \\beta_p )^T$ minimize the residual sum of squares $$ RSS(\\beta) = \\sum_{i=1}^N(y_i-f(x_i))^2 = (y-X\\beta)^T(y-X\\beta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is convex.\n",
    "\n",
    "Differentiating with respect to $\\beta$ we obtain $$\\frac{\\partial{RSS}}{\\partial{\\beta}} = -2X^T(y-X\\beta) $$\n",
    "\n",
    "If X^TX is inversible, we obtain a unique solution by setting the first derivative to 0.\n",
    "$$\\hat{\\beta} = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Remark ** It might happen that the columns of X are not linearly independent (in the case for example of two perfectly correlated inputs). Then $X^TX$ is singular and the least squares coefficients $\\hat{\\beta}$ are not uniquely defined.\n",
    "This can be avoid by dropping redundant columns in X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ Fill in the LeastSquareRegr class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquaresRegr():\n",
    "    \"\"\" Class for least-squares linear regression:\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    coef_: 1-dimensional np.array\n",
    "        coefficients of the linear regression (beta)\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        y: (num_sampes, ) np.array\n",
    "            Output vector\n",
    "        \n",
    "        Note:\n",
    "        -----\n",
    "        Updates self.coef_\n",
    "        \"\"\"\n",
    "        # Create a (num_samples, num_features+1) np.array X_aug whose first column \n",
    "        # is a column of all ones (so as to fit an intercept).\n",
    "        # TODO\n",
    "        X = np.insert(np.array(X), 0, 1, axis=1)\n",
    "        print(X.shape)\n",
    "        # Update self.coef_\n",
    "        self.coef_ = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)\n",
    "        # TODO\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Make predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        return np.dot(np.insert(X, 0, 1, axis=1), self.coef_)\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate your least squares regression on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4408, 12)\n",
      "(4408, 12)\n",
      "(4408, 12)\n",
      "(4408, 12)\n",
      "(4408, 12)\n",
      "(4408, 12)\n",
      "(4408, 12)\n",
      "(4408, 12)\n",
      "(4409, 12)\n",
      "(4409, 12)\n",
      "Mean squared error: 0.586\n"
     ]
    }
   ],
   "source": [
    "regr = LeastSquaresRegr()\n",
    "pred = cross_validate_regr(X_regr, y_regr, regr, folds_regr)\n",
    "print(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_regr,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent solution\n",
    "\n",
    "Processing the entire dataset in one go can be computationally costly for large datasets. In addition, a small change in the training set involves computing the new parameters from scratch. In both cases, it may be worthwhile to use sequential algorithms in which the datapoints are considered one at a time and the model parameters updated at each time. \n",
    "\n",
    "We will create a sequential version of our least squares regressor, using gradient descent. \n",
    "\n",
    "__Question:__ Fill in the blanks.\n",
    "\n",
    "__Hints:__ You can use [np.reshape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) to cast a 1-dimensional np.array `V` of shape `(n,)` as a 2-dimensional np.array of shape `(n, 1)` using `np.reshape(V, [-1,1])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class seq_LeastSquaresRegr():\n",
    "    \"\"\" Class for sequential least-squares linear regression:\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    coef_: 1-dimensional np.array\n",
    "        coefficients of the linear regression (beta)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        y: (num_sampes, ) np.array\n",
    "            Output vector\n",
    "        \n",
    "        Note:\n",
    "        -----\n",
    "        Updates self.coef_\n",
    "        \"\"\"\n",
    "        # Create a (num_samples, num_features+1) np.array X_aug whose first column \n",
    "        #    is a column of all ones (for beta_0). The rest of the columns are taken\n",
    "        #    from X.\n",
    "        # TODO\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        # Initialize self.coef_ at random, with shape (num_features+1,)\n",
    "        self.coef_ = np.random.rand(len(X[0]+1))\n",
    "        # TODO\n",
    "        print(self.coef_)\n",
    "         \n",
    "        def f_ls(beta):\n",
    "            \"\"\" Returns the least square error between y and X.beta.        \n",
    "            \"\"\"\n",
    "            # TODO\n",
    "            vec = y - X.dot(beta)\n",
    "            return vec.T.dot(vec)\n",
    "\n",
    "        \n",
    "        def fprime_ls(beta):\n",
    "            \"\"\" Returns the gradient of f_ls at beta.\n",
    "            IMPORTANT:  The output should have the same shape as beta,\n",
    "            otherwise our optimiser will not work.\n",
    "            \"\"\"\n",
    "            res = -2*X.T.dot((y - X.dot(beta)))\n",
    "            return res\n",
    "            # TODO\n",
    "        \n",
    "        # Use gradient descent optimization to minimize the least squares error.\n",
    "        gd = GradientDescentOptimizer(f_ls, fprime_ls, self.coef_, lr=1e-6)\n",
    "        gd.optimize(max_iter=10000)\n",
    "        \n",
    "        # Update self.coef_\n",
    "        # You must retrieve gd.beta after the optimisation is complete, and store it in\n",
    "        #    self.coef_.\n",
    "        # TODO\n",
    "        self.coef_ = gd.beta\n",
    "        print(self.coef_)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Make predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        # Do not forget to appropriately augment the data matrix!\n",
    "        # Also, do not forget the convention - the first column is all ones, while\n",
    "        #    the rest of the columns are taken from X.\n",
    "        # TODO\n",
    "        return np.insert(X, 0, 1, axis=1).dot(self.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate your sequential least squares regression on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70719691  0.44226881  0.93715486  0.17386014  0.70626559  0.8081342\n",
      "  0.54431649  0.66340156  0.37503003  0.0085345   0.63344788  0.08536154]\n",
      "[  5.88268508e+00   3.90000852e-02  -1.90008635e-01   6.09167193e-04\n",
      "   3.79757516e-01  -1.36572747e-02   6.62632245e-02  -1.94466481e-02\n",
      "  -3.78095670e-01   9.69452099e-02   6.91408366e-02   2.72716103e-01]\n",
      "[ 0.01120638  0.21899854  0.60487     0.87259896  0.63450003  0.19065066\n",
      "  0.29906151  0.55304079  0.8977842   0.07158153  0.78717058  0.66157467]\n",
      "[  5.87703226e+00   3.01003086e-02  -1.97691942e-01   4.54019673e-03\n",
      "   3.34169163e-01  -1.85587703e-03   6.68515083e-02  -1.69583070e-02\n",
      "  -3.28572925e-01   8.75503870e-02   6.15924430e-02   3.02535516e-01]\n",
      "[ 0.96270052  0.43676287  0.47434549  0.43916624  0.37937368  0.36426012\n",
      "  0.91770608  0.57835641  0.07647597  0.65720202  0.77703744  0.42986666]\n",
      "[  5.86685323e+00   3.82103024e-02  -1.92489672e-01   2.93644626e-03\n",
      "   3.82251603e-01  -1.65713149e-02   6.19785481e-02  -1.50208563e-02\n",
      "  -3.97402165e-01   9.23510399e-02   7.39783885e-02   2.58303737e-01]\n",
      "[ 0.5211829   0.15105353  0.27593351  0.53021398  0.76538017  0.31416067\n",
      "  0.65058313  0.73349652  0.19974388  0.86857994  0.71751575  0.41369981]\n",
      "[  5.87337770e+00   3.85061494e-02  -1.89863474e-01  -1.74294187e-03\n",
      "   3.73354996e-01  -5.30683891e-03   6.36529292e-02  -1.37629755e-02\n",
      "  -3.89786437e-01   8.44409357e-02   6.91309167e-02   2.64892914e-01]\n",
      "[ 0.37863911  0.24042524  0.46197937  0.32686641  0.98739889  0.17687185\n",
      "  0.96864288  0.70619078  0.27842405  0.20914253  0.48545895  0.85648417]\n",
      "[  5.88051810e+00   5.00677087e-02  -1.90755324e-01  -4.18320098e-04\n",
      "   3.97188548e-01  -9.17883258e-04   6.55100999e-02  -1.20897414e-02\n",
      "  -4.18970189e-01   9.98562540e-02   7.21203173e-02   2.53177500e-01]\n",
      "[ 0.32440566  0.29931461  0.53540525  0.21011809  0.08078149  0.24519732\n",
      "  0.03047576  0.49201849  0.53543986  0.3652601   0.61001047  0.25476198]\n",
      "[  5.88012970e+00   5.13671826e-02  -1.92917478e-01  -2.60611817e-03\n",
      "   3.79538130e-01  -6.93065802e-03   6.09439996e-02  -4.53191757e-03\n",
      "  -4.13378602e-01   9.25474313e-02   7.02208022e-02   2.54287636e-01]\n",
      "[ 0.39028998  0.34094733  0.91877017  0.60835164  0.79304058  0.33999385\n",
      "  0.90581925  0.88745133  0.66038268  0.94845036  0.66851895  0.02553402]\n",
      "[  5.87906024e+00   5.18822347e-02  -1.83649602e-01   1.19032587e-03\n",
      "   3.84109300e-01  -1.97836685e-03   6.47921426e-02  -1.24797606e-02\n",
      "  -4.15467876e-01   9.68171466e-02   6.91876855e-02   2.46560771e-01]\n",
      "[ 0.07573814  0.5283019   0.24733168  0.00769042  0.12692844  0.22194849\n",
      "  0.81720061  0.37172537  0.11871618  0.32889784  0.698523    0.56260067]\n",
      "[  5.88826910e+00   5.04995992e-02  -1.85871172e-01   4.81548364e-03\n",
      "   3.82380311e-01  -8.03575901e-03   8.27910163e-02  -2.70491896e-02\n",
      "  -4.01455212e-01   9.84404352e-02   6.68430315e-02   2.64529206e-01]\n",
      "[ 0.61451642  0.76253459  0.21919629  0.04075426  0.21790764  0.80417103\n",
      "  0.70825215  0.16763083  0.49272798  0.09336884  0.47546733  0.29673011]\n",
      "[  5.87132887e+00   2.37689156e-02  -1.83442682e-01   2.84375697e-03\n",
      "   3.46007338e-01  -1.35358498e-02   6.37246390e-02  -1.75041655e-02\n",
      "  -3.51406585e-01   8.14393774e-02   6.40781296e-02   2.83423556e-01]\n",
      "[ 0.46323313  0.52521079  0.66219183  0.38862088  0.17424548  0.92227919\n",
      "  0.25674877  0.25626283  0.46854674  0.27308316  0.18268869  0.67858231]\n",
      "[  5.87933063e+00   2.90886491e-02  -1.86952002e-01   5.56760459e-03\n",
      "   3.42649569e-01  -8.49358663e-03   6.63179346e-02  -1.31958676e-02\n",
      "  -3.34472265e-01   8.13647636e-02   6.72912059e-02   2.93619486e-01]\n",
      "[ 5.56872828  5.22563487  5.75267971 ...,  5.34074662  6.55558183\n",
      "  6.34797655]\n",
      "------------\n",
      "Mean squared error: 0.567\n"
     ]
    }
   ],
   "source": [
    "regr = seq_LeastSquaresRegr()\n",
    "pred = cross_validate_regr(X_regr, y_regr, regr, folds_regr)\n",
    "print(pred)\n",
    "print('------------')\n",
    "print(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_regr, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question ** Discuss the difference of errors between the two implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### scikit-learn implementation\n",
    "\n",
    "We will now compare to the scikit-learn implementation.\n",
    "\n",
    "__Question__ Cross-validate scikit-learn's [linear_model.LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.569\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize a LinearRegression model\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Cross-validate it\n",
    "pred = cross_validate_regr(X_regr, y_regr, regr, folds_regr)\n",
    "print(\"Mean squared error: %.3f\" % metrics.mean_squared_error(y_regr, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Logistic regression\n",
    "\n",
    "We will now implement a linear regression, first using the closed form solution, and second with our gradient descent.\n",
    "\n",
    "## 2.1 Logistic regression data\n",
    "\n",
    "Our second data set comes from the world of bioinformatics. In this data set, each observation is a tumor, and it is described by the expression of 3,000 genes. The expression of a gene is a measure of how much of that gene is present in the biological sample. Because this affects how much of the protein this gene codes for is produced, and because proteins dictacte what cells can do, gene expression gives us valuable information about the tumor. In particular, the expression of the same gene in the same individual is different in different tissues (although the DNA is the same): this is why blood cells look different from skin cells. In our data set, there are two types of tumors: breast tumors and ovary tumors. Let us see if gene expression can be used to separate them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classification task data\n",
    "breast_data = pd.read_csv('data/small_Breast_Ovary.csv')\n",
    "\n",
    "# Drop the 'Tissue' column to create the design matrix\n",
    "X_clf = np.array(breast_data.drop(['Tissue'], axis=1).values)\n",
    "\n",
    "# Use the 'Tissue' column to create the labels (0=Breast, 1=Ovary)\n",
    "y_clf = np.array(breast_data['Tissue'].values)\n",
    "y_clf[np.where(y_clf == 'Breast')] = 0\n",
    "y_clf[np.where(y_clf == 'Ovary')] = 1\n",
    "y_clf = y_clf.astype(np.int)\n",
    "\n",
    "#sc = preprocessing.StandardScaler()\n",
    "#sc.fit(X_clf)\n",
    "#X_clf = sc.transform(X_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ How many samples do we have? How many belong to each class? How many features do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Cross-validation\n",
    "\n",
    "Let us create a cross-validation utility function (similar to what we have done in Lab 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up folds for cross_validation\n",
    "from sklearn import cross_validation\n",
    "folds_clf = cross_validation.StratifiedKFold(y_clf, n_folds=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_clf(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        classifier.fit(design_matrix[tr,:], labels[tr])\n",
    "        pred[te] = classifier.predict_proba(design_matrix[te,:])[:,1]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Logistic regression implementation\n",
    "\n",
    "Under the assumption of linear boundaries segragating classes, the posterior probability of class $C_1$ can be written as a logistic sigmoid acting on a linear function of the feature vector X so that $p(C_1|X) = \\sigma(\\beta^TX)$.\n",
    "\n",
    "For an input vector $X^T = (X_1 , X_2 , \\dots , X_p )$, and a binary output Y such that $y_i\\in\\{0,1\\}$, the likelihood function is\n",
    "$$ p(y\\vert\\beta) = \\prod_{i=0}^Np(y_i\\vert\\beta) = \\prod_{i=0}^Np(C_1\\vert X)^{y_i}(1-p(C_1\\vert X))^{1-y_i} = \\prod_{i=0}^N\\sigma(\\beta^TX)^{y_i}(1-\\sigma(\\beta^TX))^{1-y_i}.$$\n",
    "\n",
    "We can then define (as usual) an error function to minimize by taking the negative logarithm of the likelihood\n",
    "$$ E_{loss}(\\beta) = -\\log(p(y|\\beta)) = -\\sum_{i=0}^N\\{ y_i\\log(\\sigma(\\beta^Tx_i)) + (1-y_i)\\log(1-\\sigma(\\beta^Tx_i)) \\} = \\sum_{i=0}^N\\log(e^{-y_i*x_i^T\\beta}+1)$$\n",
    "\n",
    "Then it's derivative can be written $$\\nabla E(\\beta) = \\sum_{i=1}^N(\\sigma(x_i^T\\beta)-y_i)x_i.$$\n",
    "\n",
    "__Question:__ Now that you know how to write the loss function for logistic regression, `f(beta)`, and what its derivative `f'(beta)` is in terms of `beta`, use the previously defined `GradientDescentOptimizer` to fit a logistic regression model to our data `(X, y)`. Some of the code below has already been filled to help you. \n",
    "\n",
    "**Remark:** The derivative of logistic function $\\sigma(x)=\\frac{1}{1+e^{-x}}$ can be written $\\text{d}\\sigma = \\sigma (1-\\sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: np.array\n",
    "        input variables\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    r: np.array of same dimension as x\n",
    "        outputs\n",
    "    \"\"\"\n",
    "    # Truncate the very large and very small values of x to avoid overflowing the exponential.\n",
    "    x[np.where(x > 7e2)] = 7e2\n",
    "    x[np.where(x < -7e2)] = -7e2\n",
    "    \n",
    "    # Compute the sigmoid\n",
    "    r = 1. / (1 + np.exp(-1*x))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-3828537b3793>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-3828537b3793>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    c1_ids = # TODO\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegr():\n",
    "    \"\"\" Class for sequential least-squares linear regression:\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    coef_: 1-dimensional np.array\n",
    "        coefficients of the linear regression (beta)\n",
    "    classes: list\n",
    "        list of class labels\n",
    "    \"\"\"\n",
    "    def __init__(self,):\n",
    "        self.coef_ = None\n",
    "        self.classes_ = [0,1]\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the data (X, y).\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        y: (num_sampes, ) np.array\n",
    "            Output vector\n",
    "        \n",
    "        Note:\n",
    "        -----\n",
    "        Updates self.coef_\n",
    "        \"\"\"\n",
    "        # Create a (num_samples, num_features+1) np.array X_aug whose first column \n",
    "        #    is a column of all ones (for beta_0), while the remaining columns are \n",
    "        #    taken from X.\n",
    "        # TODO\n",
    "        \n",
    "        # Initialize self.coef_ at random, with the right shape\n",
    "        # TODO\n",
    "         \n",
    "        # Find the indices which correspond to class 0\n",
    "        c0_ids = np.where(y == 0)[0] \n",
    "        # Find the indices which correspond to class 1. \n",
    "        c1_ids = # TODO\n",
    "            \n",
    "        def f_lr(beta):            \n",
    "            \"\"\" \n",
    "            Returns the logistic loss between y and X.beta.        \n",
    "            \"\"\"\n",
    "            # Reshape beta as a two-dimensional array with second dimension equal to 1\n",
    "            beta  = # TODO\n",
    "            \n",
    "            # Compute phi_0, the model's prediction for class 0 samples\n",
    "            phi_0 = # TODO\n",
    "            \n",
    "            # Compute phi_1, the model's prediction for class 1 samples\n",
    "            phi_1 = # TODO\n",
    "            \n",
    "            # Compute the loss over class 1 samples\n",
    "            loss = # TODO\n",
    "            # Update the loss with the loss over class 0 samples\n",
    "            loss = # TODO\n",
    "            return loss\n",
    "        \n",
    "        def fprime_lr(beta):   \n",
    "            \"\"\" Returns the gradient of f_ls at beta.\n",
    "            IMPORTANT:  The output should have the same shape as beta,\n",
    "            otherwise our optimiser will not work!\n",
    "            \"\"\"\n",
    "            # TODO\n",
    "        \n",
    "        # Now optimize the loss over the data:\n",
    "        # Use GradientDescentOptmizer to update self.coef_\n",
    "        # TODO\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Make probabilistic predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions (probabilities of belonging to class 1)\n",
    "        \"\"\"\n",
    "        # IMPORTANT: Do not forget to augment the data matrix X in the same\n",
    "        #    fashion as you did for the fit method.\n",
    "        # TODO\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Make binary predictions for data X.\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (num_samples, num_features) np.array\n",
    "            Design matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        y_pred: (num_samples, ) np.array\n",
    "            Predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate your logistic regression on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegr()\n",
    "pred = cross_validate_clf(X_clf, y_clf, clf, folds_clf)\n",
    "print(\"Accuracy: %.3f\" % metrics.accuracy_score(y_clf, pred > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### scikit-learn implementation\n",
    "\n",
    "We will now compare to the scikit-learn implementation.\n",
    "\n",
    "__Question__ Cross-validate scikit-learn's [linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize a LogisticRegression model. \n",
    "# Use C=1e7 to ensure there is no regularization (we'll talk about regularization next time!)\n",
    "clf = # TODO\n",
    "\n",
    "# Cross-validate it\n",
    "ypred_logreg = # TODO\n",
    "\n",
    "print(\"Accuracy: %.3f\" % metrics.accuracy_score(ypred_logreg > 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question : ** Plot the ROC curve. Use plt.semilogx to use a logarithmic scale on the x-axis. This \"spreads out\" the curve a little, making it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_logreg, tpr_logreg, thresholds = # TODO\n",
    "auc_logreg = # TODO\n",
    "\n",
    "plt.semilogx(fpr_logreg, tpr_logreg, '-', color='orange', \n",
    "             label='AUC = %0.3f' % auc_logreg)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve: Logistic regression', fontsize=16)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scaling\n",
    "See [preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Scale the data, and compute the cross-validated predictions of the logistic regression on the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Scale the data with preprocessing.StandardScaler\n",
    "# Initialize a scaler\n",
    "scaler = # TODO\n",
    "# Scale your design matrix\n",
    "X_clf_scaled = # TODO\n",
    "\n",
    "# Initialize a LogisticRegression model. \n",
    "# Use C=1e7 to ensure there is no regularization (we'll talk about regularization next time!)\n",
    "clf = # TODO\n",
    "\n",
    "# Cross-validate it for the scaled data\n",
    "ypred_logreg_scaled = # TODO\n",
    "\n",
    "print(\"Accuracy: %.3f\" % metrics.accuracy_score(ypred_logreg_scaled > 0.5, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Plot the two ROC curves (one for the logistic regression on the original data, one for the logistic regression on the scaled data) on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_logreg_scaled, tpr_logreg_scaled, thresholds = # TODO\n",
    "auc_logreg_scaled = # TODO\n",
    "\n",
    "plt.semilogx(fpr_logreg_scaled, tpr_logreg_scaled, '-', color='blue', \n",
    "             label='scaled data; AUC = %0.3f' % auc_logreg_scaled)\n",
    "plt.semilogx(fpr_logreg, tpr_logreg, '-', color='orange', \n",
    "             label='original data; AUC = %0.3f' % auc_logreg)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve: Logistic regression', fontsize=16)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a cross-validation setting, we ignore the samples from the test fold when training the classifier. This also means that scaling should be done on the training data only. \n",
    "\n",
    "In scikit-learn, we can use a scaler to make centering and scaling happen independently on each feature by computing the relevant statistics on the samples *in the training set*. \n",
    "The mean and standard deviation will be stored to be used on the test data.\n",
    "\n",
    "**Question** Rewrite the cross_validate method to include a scaling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_clf_with_scaling(design_matrix, labels, classifier, cv_folds):\n",
    "    \"\"\" Perform a cross-validation and returns the predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    design_matrix: (n_samples, n_features) np.array\n",
    "        Design matrix for the experiment.\n",
    "    labels: (n_samples, ) np.array\n",
    "        Vector of labels.\n",
    "    classifier:  sklearn classifier object\n",
    "        Classifier instance; must have the following methods:\n",
    "        - fit(X, y) to train the classifier on the data X, y\n",
    "        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n",
    "    cv_folds: sklearn cross-validation object\n",
    "        Cross-validation iterator.\n",
    "        \n",
    "    Return:\n",
    "    -------\n",
    "    pred: (n_samples, ) np.array\n",
    "        Vectors of predictions (same order as labels).\n",
    "    \"\"\"\n",
    "    pred = np.zeros(labels.shape)\n",
    "    for tr, te in cv_folds:\n",
    "        # TODO\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Now use the cross_validate_with_scaling method to cross-validate the logistic regression on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = linear_model.LogisticRegression(C=1e6) \n",
    "ypred_logreg_scaled_ = cross_validate_clf_with_scaling(X_clf, y_clf, clf, folds_clf)\n",
    "print(metrics.accuracy_score(y_clf,np.where(ypred_logreg_scaled_ > 0.5, 1, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Again, compare the AUROC and ROC curves with those obtained previously. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_logreg_scaled_, tpr_logreg_scaled_, thresholds = # TODO\n",
    "auc_logreg_scaled_ = # TODO\n",
    "\n",
    "plt.semilogx(fpr_logreg_scaled, tpr_logreg_scaled, '-', \n",
    "             color='blue', label='scaled data overfit; AUC = %0.3f' % auc_logreg_scaled)\n",
    "plt.semilogx(fpr_logreg, tpr_logreg, '-', color='orange', \n",
    "             label='original data; AUC = %0.3f' % auc_logreg)\n",
    "plt.semilogx(fpr_logreg_scaled_, tpr_logreg_scaled_, '-', color='black', \n",
    "             label='scaled data no overfit; AUC = %0.3f' % auc_logreg_scaled_)\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve: Logistic regression', fontsize=16)\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kaggle challenge ideas\n",
    "\n",
    "* Load the data\n",
    "* Set up a cross-validation that you will use for all your evaluations. Notice there is a 'random_state' parameter to the cross-validation methods of scikit-learn, that you can use to ensure you always get the same splits.\n",
    "* To go one step further in ensuring a fair comparison of your algorithms, you can use multiple repeats of the cross-validation procedure (using different splits each time), and report the mean & standard deviation over the repeats of the performance obtained. If you do this, you can report standard deviations in plots by using error bars.\n",
    "* Evaluate the performance of a linear regression on your data. Which evaluation metric are you using? See http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics for help defining one.\n",
    "* Submit a linear regression predictor to the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
